{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whorseman/Assignments/blob/main/homework_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following assignment consists again of a theoretical part (learning portfolio) and a practical part (assignment). The goal is to train a neural model from scratch, as we did a few weeks ago. I would like us to repeat the most important things so that we can consolidate our knowledge in this area, for this you will train a model on a dataset that I will provide via OLAT.\n",
        "\n",
        "The plan would be that in the first we will discuss your learnings from the theory part, that means you are relatively free to fill your Learning Portfolio on this topic and in the following week we will discuss your solutions of the Classification Model.\n",
        "\n",
        "#Theory part (filling your Learning Portfolio, May 24)\n",
        "\n",
        "In preparation for the practical part, I ask you to familiarize yourself with the following video sources in the next week:\n",
        "\n",
        "1) Please watch the following video until random forests:\n",
        "\n",
        "https://course.fast.ai/Lessons/lesson5.html\n",
        "\n",
        "2) Please download the following notebooks and edit it in Google-Colab. Take notes and update your Learning Portfolio.\n",
        "\n",
        "https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch\n",
        "\n",
        "https://www.kaggle.com/code/jhoward/why-you-should-use-a-framework\n",
        "\n",
        "The contents should be mostly known to you, because these contents are also based on chapter 4, which we have already worked on. We repeat the basics again with a new data set to consolidate our knowledge.\n",
        "\n",
        "\n",
        "\n",
        "#Practical part (Assignment, May 31)\n",
        "\n",
        "The following task is a binary classification task. The first column is our target. All variables are categorical variables from which you have to create dummy variables. The target column has only the property Q or F, which is to be modeled. As always, please create a train and a test data set, e.g. 80:20 or 90:10. Your goal is to create a neural network that best predicts the target column. Use probabilities with the sigmoid function as discussed in the theory. When designing your neural network, play with a few different numbers and sizes of layers and different activation functions.\n",
        "\n",
        "To validate your model, you can compare your results with a logistic regression model. (Note: This data set should allow you to achieve very high accuracies: > 98% maybe even >99%) You can see how these results vary when you use 50% for testing and 50% for training."
      ],
      "metadata": {
        "id": "PpUcmUeU4KBV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTf6JM9C4G2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0b0b31-629f-4dc4-984e-4bc9ffcf09b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import data_table\n",
        "import matplotlib.pyplot as plt\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we have discussed the assignment, I will tell you what the data set is about, which has its origin from our nature."
      ],
      "metadata": {
        "id": "ddukAPmHp-P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data  = pd.read_csv(\"/content/drive/MyDrive/DIGO/data_homework_6.csv\")\n",
        "data = data.drop(columns=['variable_16'])\n",
        "doh   = pd.get_dummies(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "NQ6Se-zvCmEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cols = list(data)\n",
        "\n",
        "df2 = np.unique(data[cols].values)\n",
        "\n",
        "for col in data:\n",
        "    print(data[col].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQRL7t7wVdq_",
        "outputId": "0caa76cd-11c8-4ac1-8628-19813910da0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Q' 'F']\n",
            "['Y' 'C' 'T' 'G' 'L' 'D']\n",
            "['T' 'Z' 'G' 'H']\n",
            "['O' 'Z' 'X' 'H' 'F' 'Q' 'C' 'V' 'D' 'S']\n",
            "['U' 'G']\n",
            "['Q' 'B' 'M' 'O' 'G' 'D' 'Z' 'T' 'N']\n",
            "['G' 'B']\n",
            "['D' 'X']\n",
            "['O' 'C']\n",
            "['L' 'O' 'H' 'Q' 'X' 'I' 'V' 'F' 'C' 'S' 'Z' 'P']\n",
            "['F' 'U']\n",
            "['F' 'D' 'C' 'S' 'Z']\n",
            "['T' 'G' 'L' 'Z']\n",
            "['T' 'G' 'Z' 'L']\n",
            "['X' 'H' 'Q' 'O' 'C' 'F' 'P' 'D' 'Z']\n",
            "['X' 'Q' 'H' 'C' 'O' 'F' 'Z' 'P' 'D']\n",
            "['X' 'O' 'P' 'Z']\n",
            "['P' 'U' 'O']\n",
            "['Q' 'F' 'M' 'G' 'O']\n",
            "['L' 'O' 'V' 'I' 'X' 'S' 'P' 'Z' 'C']\n",
            "['T' 'O' 'B' 'W' 'Z' 'D']\n",
            "['V' 'H' 'N' 'E' 'Q' 'X' 'M']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating independant and dependatn variable tensors\n",
        "from torch import tensor\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "t_dep = tensor(doh.target_F)\n",
        "dohp = doh.drop(columns=['target_F','target_Q'])\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') # don't have GPU\n",
        "    return device\n",
        "\n",
        "# convert a df to tensor to be used in pytorch\n",
        "def df_to_tensor(df):\n",
        "    device = get_device()\n",
        "    return torch.from_numpy(df.values).float().to(device)\n",
        "\n",
        "t_indep = df_to_tensor(dohp)"
      ],
      "metadata": {
        "id": "PxUGXilADHXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testsplit of training data\n",
        "from fastai.data.transforms import RandomSplitter\n",
        "trn_split,val_split=RandomSplitter(seed=42)(doh)\n",
        "\n",
        "trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\n",
        "trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\n",
        "len(trn_indep),len(val_indep)\n",
        "\n",
        "trn_dep = trn_dep[:,None]\n",
        "val_dep = val_dep[:,None]"
      ],
      "metadata": {
        "id": "TMtgMmF7JIXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qKH_jWcvZmk",
        "outputId": "819c4852-be0d-4943-c8d2-3476fa95fca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0],\n",
              "        [1],\n",
              "        [1],\n",
              "        ...,\n",
              "        [1],\n",
              "        [1],\n",
              "        [1]], dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()\n",
        "\n",
        "def one_epoch(coeffs, lr):\n",
        "    loss = calc_loss(coeffs, trn_indep, trn_dep)\n",
        "    loss.backward()\n",
        "    with torch.no_grad(): update_coeffs(coeffs, lr)\n",
        "    print(f\"{loss:.3f}\", end=\"; \")\n",
        "\n",
        "def train_model(epochs=30, lr=0.01):\n",
        "    torch.manual_seed(442)\n",
        "    coeffs = init_coeffs()\n",
        "    for i in range(epochs): one_epoch(coeffs, lr=lr)\n",
        "    return coeffs\n",
        "\n",
        "def update_coeffs(coeffs, lr):\n",
        "    layers,consts = coeffs\n",
        "    for layer in layers+consts:\n",
        "        layer.sub_(layer.grad * lr)\n",
        "        layer.grad.zero_()\n",
        "\n",
        "def init_coeffs():\n",
        "    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n",
        "    sizes = [n_coeff] + hiddens + [1]\n",
        "    n = len(sizes)\n",
        "    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n",
        "    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n",
        "    for l in layers+consts: l.requires_grad_()\n",
        "    return layers,consts\n",
        "\n",
        "def calc_preds(coeffs, indeps):\n",
        "    layers,consts = coeffs\n",
        "    n = len(layers)\n",
        "    res = indeps\n",
        "    for i,l in enumerate(layers):\n",
        "        res = res@l + consts[i]\n",
        "        if i!=n-1: res = F.relu(res)\n",
        "    return torch.sigmoid(res)"
      ],
      "metadata": {
        "id": "Rkoq9JE3Krp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_coeff = t_indep.shape[1]\n",
        "coeffs = torch.rand(n_coeff)-0.5\n",
        "\n",
        "coeffs = train_model(epochs=300,lr=0.234)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW_ZG2NlMtwB",
        "outputId": "7289cbdc-008b-4111-df67-a26cbbf494cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.484; 0.484; 0.483; 0.483; 0.482; 0.480; 0.473; 0.432; 0.489; 0.384; 0.513; 0.511; 0.507; 0.484; 0.297; 0.302; 0.484; 0.484; 0.484; 0.484; 0.484; 0.484; 0.483; 0.483; 0.483; 0.482; 0.481; 0.478; 0.465; 0.346; 0.429; 0.195; 0.192; 0.169; 0.134; 0.110; 0.102; 0.095; 0.087; 0.080; 0.073; 0.067; 0.062; 0.057; 0.054; 0.050; 0.048; 0.045; 0.043; 0.042; 0.040; 0.039; 0.037; 0.036; 0.035; 0.034; 0.034; 0.033; 0.032; 0.031; 0.031; 0.030; 0.029; 0.029; 0.028; 0.028; 0.027; 0.027; 0.026; 0.026; 0.026; 0.025; 0.025; 0.024; 0.024; 0.024; 0.023; 0.023; 0.023; 0.022; 0.022; 0.022; 0.021; 0.021; 0.021; 0.021; 0.020; 0.020; 0.020; 0.019; 0.019; 0.019; 0.019; 0.019; 0.018; 0.018; 0.018; 0.018; 0.018; 0.017; 0.017; 0.017; 0.017; 0.017; 0.016; 0.016; 0.016; 0.016; 0.016; 0.016; 0.015; 0.015; 0.015; 0.015; 0.015; 0.014; 0.014; 0.014; 0.014; 0.014; 0.014; 0.013; 0.013; 0.013; 0.013; 0.013; 0.013; 0.012; 0.012; 0.012; 0.012; 0.012; 0.012; 0.011; 0.011; 0.011; 0.011; 0.011; 0.011; 0.010; 0.010; 0.010; 0.010; 0.010; 0.010; 0.010; 0.010; 0.009; 0.009; 0.009; 0.009; 0.009; 0.009; 0.009; 0.009; 0.008; 0.008; 0.008; 0.008; 0.008; 0.008; 0.008; 0.008; 0.008; 0.008; 0.008; 0.008; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.007; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.006; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.005; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.004; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; 0.003; "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\n",
        "\n",
        "acc(coeffs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9WRvltjOMbO",
        "outputId": "b422f9b7-512d-4f0f-8dd4-2c6c6c4b0599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9994)"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    }
  ]
}