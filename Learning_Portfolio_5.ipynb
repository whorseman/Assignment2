{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP07lTLGCitzN8RFT1SCJfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whorseman/Assignments/blob/main/Learning_Portfolio_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrOd45E1-E48"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pretrained NLP is a model that is already finetuned to process language in a certain mannner, e.g. sentiment analysis by looking for buzzwords like 'good'. Fine tuning a model would mean fitting it to a data set for a specific task, while it has been pretrained on a general larges corpus of data. NLP tasks are often completed with a Transformer model. The general setup of such a model is devided into the encoder and decoder block. The encoder is given all input at once and produces a classification or just the predominant features of the type of text given to it. The decoder than takes those features and produces which words are most likely to occure next. While the aforementioned procedure is somewhat specific it seems to describe the general architecture: The encoder takes the whole text and says what is about, the decoder tries to predict what comes afterwards. The training of the two blocks is accordingly: The encoder gets fed the whole text with some parts distorted trying to undo those distortion. The decoder on the other hand gets the token up until a certain word and tries to predict the next. "
      ],
      "metadata": {
        "id": "_N6Ha0iQ-FqO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0LaupVSceNiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}