{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whorseman/Assignments/blob/main/homework_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory\n",
        "\n",
        "In the following assignment, your task is to complete the MNIST Basics chapter. It is best to repeat everything from last week and try to answer the following questions. Afterwards you have to summarize the learned facts with two programming tasks.\n",
        "\n",
        "What is \"torch.cat()\" and \".view(-1, 28*28)\" doing in the beginning of the \"The MNIST Loss Function\" chapter?\n",
        "\n",
        "Can you draw the neuronal network, which is manually trained in chapter \"The MNIST Loss Function\"?\n",
        "\n",
        "Why is it not possible to use the accuracy as loss function?\n",
        "\n",
        "What is the defined `mnist_loss` function doing? \n",
        "\n",
        "\n",
        "```\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "```\n",
        "\n",
        "Why do we need additionaly the sigmoid() function? What is it technically in our TLU?\n",
        "\n",
        "Again, what are mini batches, why are we using them and why should they be shuffeld? "
      ],
      "metadata": {
        "id": "iIBgQ5f43H6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.cat concentrates all images in a single tensor. They are changed from matrices to a list of vectors with .view. The -1 in .view makes sure that the vectors are as big as necessary to fit the whole axis in."
      ],
      "metadata": {
        "id": "coTbL06PTgfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()\n",
        "\n",
        "#hide\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQy6mBWsJJia",
        "outputId": "3ffb3b2b-765f-4ba1-e0cc-352421eac137"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "Path.BASE_PATH = path\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "TU4sqwMzJ4Xa",
        "outputId": "b968997c-2ae1-43ae-e494-4766f2346763"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.14% [3219456/3214948 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
        "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
        "stacked_threes = torch.stack(three_tensors).float()/255\n",
        "\n",
        "valid_3_tens = torch.stack([tensor(Image.open(o)) \n",
        "                            for o in (path/'valid'/'3').ls()])\n",
        "valid_3_tens = valid_3_tens.float()/255\n",
        "valid_7_tens = torch.stack([tensor(Image.open(o)) \n",
        "                            for o in (path/'valid'/'7').ls()])\n",
        "valid_7_tens = valid_7_tens.float()/255\n",
        "\n",
        "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
        "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
        "\n",
        "dset = list(zip(train_x,train_y))\n",
        "\n",
        "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
        "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
        "valid_dset = list(zip(valid_x,valid_y))"
      ],
      "metadata": {
        "id": "xIOzcPTv1SrX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drawing the neural net \n",
        "\n",
        "model = nn.Linear(28*28,1)\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot\n",
        "\n",
        "make_dot(model(train_x).mean(), params=dict(model.named_parameters()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "4ilxLks9fLOx",
        "outputId": "cb5f4fbb-c72e-4513-d689-d5bd720e9e37"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"216pt\" height=\"336pt\"\n viewBox=\"0.00 0.00 216.00 336.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 332)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-332 212,-332 212,4 -4,4\"/>\n<!-- 140545611382304 -->\n<g id=\"node1\" class=\"node\">\n<title>140545611382304</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"130.5,-31 76.5,-31 76.5,0 130.5,0 130.5,-31\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 140545627510512 -->\n<g id=\"node2\" class=\"node\">\n<title>140545627510512</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 56,-86 56,-67 151,-67 151,-86\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n</g>\n<!-- 140545627510512&#45;&gt;140545611382304 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140545627510512&#45;&gt;140545611382304</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-66.79C103.5,-60.07 103.5,-50.4 103.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-41.19 103.5,-31.19 100,-41.19 107,-41.19\"/>\n</g>\n<!-- 140545627514640 -->\n<g id=\"node3\" class=\"node\">\n<title>140545627514640</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154,-141 53,-141 53,-122 154,-122 154,-141\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 140545627514640&#45;&gt;140545627510512 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140545627514640&#45;&gt;140545627510512</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-121.75C103.5,-114.8 103.5,-104.85 103.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-96.09 103.5,-86.09 100,-96.09 107,-96.09\"/>\n</g>\n<!-- 140545627516944 -->\n<g id=\"node4\" class=\"node\">\n<title>140545627516944</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 140545627516944&#45;&gt;140545627514640 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140545627516944&#45;&gt;140545627514640</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.25,-176.75C66.97,-169.03 78.4,-157.6 87.72,-148.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"90.31,-150.64 94.91,-141.09 85.36,-145.69 90.31,-150.64\"/>\n</g>\n<!-- 140545606442480 -->\n<g id=\"node5\" class=\"node\">\n<title>140545606442480</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-262 23.5,-262 23.5,-232 77.5,-232 77.5,-262\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 140545606442480&#45;&gt;140545627516944 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140545606442480&#45;&gt;140545627516944</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.84C50.5,-224.21 50.5,-214.7 50.5,-206.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.27 50.5,-196.27 47,-206.27 54,-206.27\"/>\n</g>\n<!-- 140545627518768 -->\n<g id=\"node6\" class=\"node\">\n<title>140545627518768</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196,-196 119,-196 119,-177 196,-177 196,-196\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 140545627518768&#45;&gt;140545627514640 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140545627518768&#45;&gt;140545627514640</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.58,-176.75C140.72,-169.03 129.07,-157.6 119.58,-148.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"121.84,-145.6 112.25,-141.09 116.94,-150.59 121.84,-145.6\"/>\n</g>\n<!-- 140545627515024 -->\n<g id=\"node7\" class=\"node\">\n<title>140545627515024</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-256.5 107,-256.5 107,-237.5 208,-237.5 208,-256.5\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 140545627515024&#45;&gt;140545627518768 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140545627515024&#45;&gt;140545627518768</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-237.37C157.5,-229.25 157.5,-216.81 157.5,-206.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-206.17 157.5,-196.17 154,-206.17 161,-206.17\"/>\n</g>\n<!-- 140545606441600 -->\n<g id=\"node8\" class=\"node\">\n<title>140545606441600</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"193,-328 122,-328 122,-298 193,-298 193,-328\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">weight</text>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (1, 784)</text>\n</g>\n<!-- 140545606441600&#45;&gt;140545627515024 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140545606441600&#45;&gt;140545627515024</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-297.8C157.5,-288.7 157.5,-276.79 157.5,-266.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-266.84 157.5,-256.84 154,-266.84 161,-266.84\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fd35433f370>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function returns the mean of the distance between the predictions and the target 1. \n"
      ],
      "metadata": {
        "id": "HGkHc5zChQeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need the sigmoid as a differntiable function that returns a value between 0 and 1 and responds to small changes in the input. Mini batches are chunks of the data the model is trained on. They are useful as they efficently use the resources at hand. That means they give the computer (usually the GPU) a size of data it can perform the gradient decent upon. The batches are shuffled to avoid clustering and bias. "
      ],
      "metadata": {
        "id": "AsN_zibukZWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Part\n",
        "\n",
        "Try to understand all parts of the code needed to manually train a single TLU/Perceptron, so use and copy all parts of the code from \"First Try: Pixel Similarity\" to the \"Putting it all together\" chapter. In the second step, use an optimizer, a second layer, and a ReLU as a hidden activation function to train a simple neural network. When copying the code, think carefully about what you really need and how you can summarize it as compactly as possible. (Probably each attempt requires about 15 lines of code.)"
      ],
      "metadata": {
        "id": "aoQq7z5D3XXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxwyNuzj3DYu"
      },
      "outputs": [],
      "source": [
        "#YOUR TASK: Manually train a single layer perceptron without using an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize parameters\n",
        "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
        "\n",
        "#loss function\n",
        "def mnist_loss(predictions, targets):\n",
        "    predictions = predictions.sigmoid()\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "\n",
        "#batch accuracy\n",
        "def batch_accuracy(xb, yb):\n",
        "    preds = xb.sigmoid()\n",
        "    correct = (preds>0.5) == yb\n",
        "    return correct.float().mean()\n",
        "\n",
        "def calc_grad(xb, yb, model):\n",
        "    preds = model(xb)\n",
        "    loss = mnist_loss(preds, yb)\n",
        "    loss.backward()\n",
        "\n",
        "def train_epoch(model, lr, params):\n",
        "    for xb,yb in dl:\n",
        "        calc_grad(xb, yb, model)\n",
        "        for p in params:\n",
        "            p.data -= p.grad*lr\n",
        "            p.grad.zero_()\n",
        "\n",
        "def validate_epoch(model):\n",
        "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
        "    return round(torch.stack(accs).mean().item(), 4)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "VsPi1Ztn9Vbz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataloader\n",
        "dl = DataLoader(dset, batch_size=256)\n",
        "xb,yb = first(dl)\n",
        "\n",
        "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
        "\n",
        "#initialize weights\n",
        "weights = init_params((28*28,1))\n",
        "bias = init_params(1)\n",
        "\n",
        "\n",
        "#define linear model\n",
        "def linear1(xb): return xb@weights + bias"
      ],
      "metadata": {
        "id": "6KDwnU0OIoLB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learning rate and parameters\n",
        "lr = 1.\n",
        "params = weights,bias\n",
        "\n",
        "for i in range(20):\n",
        "    train_epoch(linear1, lr, params)\n",
        "    print(validate_epoch(linear1), end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q17J1ggMIvpE",
        "outputId": "8303ae89-3484-4a0a-8476-0763c2f4856e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6185 0.7958 0.9081 0.9413 0.9521 0.9585 0.9643 0.9677 0.9682 0.9702 0.9731 0.9731 0.9741 0.9741 0.9746 0.9751 0.976 0.976 0.9765 0.9765 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR TASK: Train a simple two-layer neural network (two perceptrons + hidden activation function) with built-in functions and an optimizer."
      ],
      "metadata": {
        "id": "UGsLRFtMbyRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating an Optimizer**"
      ],
      "metadata": {
        "id": "kOJbnUShAGVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = nn.Linear(28*28,1)\n",
        "w,b = linear_model.parameters()\n",
        "\n"
      ],
      "metadata": {
        "id": "LeJo-jDTAJ-n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Put in dl and Datasets\n",
        "\n",
        "simple_net = nn.Sequential(\n",
        "    nn.Linear(28*28,30),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(30,1)\n",
        ")\n",
        "\n",
        "dls = DataLoaders(dl, valid_dl)\n",
        "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
        "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
        "\n",
        "learn.fit(40, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SacS73GTA3F5",
        "outputId": "3fae9413-fe50-4f7f-a8b8-c40327f6e395"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>batch_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.240010</td>\n",
              "      <td>0.374979</td>\n",
              "      <td>0.538273</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.129064</td>\n",
              "      <td>0.190689</td>\n",
              "      <td>0.859176</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.086836</td>\n",
              "      <td>0.111107</td>\n",
              "      <td>0.934249</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.067068</td>\n",
              "      <td>0.082191</td>\n",
              "      <td>0.951423</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.056598</td>\n",
              "      <td>0.068186</td>\n",
              "      <td>0.962218</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.050389</td>\n",
              "      <td>0.060007</td>\n",
              "      <td>0.963199</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.046301</td>\n",
              "      <td>0.054635</td>\n",
              "      <td>0.965653</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.043361</td>\n",
              "      <td>0.050816</td>\n",
              "      <td>0.967125</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.041100</td>\n",
              "      <td>0.047946</td>\n",
              "      <td>0.967125</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.039277</td>\n",
              "      <td>0.045699</td>\n",
              "      <td>0.968597</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.037756</td>\n",
              "      <td>0.043882</td>\n",
              "      <td>0.969087</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.036456</td>\n",
              "      <td>0.042377</td>\n",
              "      <td>0.969087</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.035327</td>\n",
              "      <td>0.041104</td>\n",
              "      <td>0.970559</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.034331</td>\n",
              "      <td>0.040009</td>\n",
              "      <td>0.971050</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.033445</td>\n",
              "      <td>0.039054</td>\n",
              "      <td>0.971541</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.032648</td>\n",
              "      <td>0.038211</td>\n",
              "      <td>0.972031</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.031926</td>\n",
              "      <td>0.037460</td>\n",
              "      <td>0.972031</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.031269</td>\n",
              "      <td>0.036784</td>\n",
              "      <td>0.972031</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.030667</td>\n",
              "      <td>0.036171</td>\n",
              "      <td>0.971541</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.030112</td>\n",
              "      <td>0.035611</td>\n",
              "      <td>0.971541</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.029599</td>\n",
              "      <td>0.035097</td>\n",
              "      <td>0.971541</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.029123</td>\n",
              "      <td>0.034623</td>\n",
              "      <td>0.972522</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.028679</td>\n",
              "      <td>0.034183</td>\n",
              "      <td>0.974485</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.028264</td>\n",
              "      <td>0.033773</td>\n",
              "      <td>0.974485</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.027875</td>\n",
              "      <td>0.033391</td>\n",
              "      <td>0.974975</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.027510</td>\n",
              "      <td>0.033031</td>\n",
              "      <td>0.975466</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.027165</td>\n",
              "      <td>0.032693</td>\n",
              "      <td>0.975957</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.026840</td>\n",
              "      <td>0.032375</td>\n",
              "      <td>0.975957</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.026532</td>\n",
              "      <td>0.032073</td>\n",
              "      <td>0.975957</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.026240</td>\n",
              "      <td>0.031787</td>\n",
              "      <td>0.975957</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.025963</td>\n",
              "      <td>0.031516</td>\n",
              "      <td>0.975957</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.025698</td>\n",
              "      <td>0.031257</td>\n",
              "      <td>0.976448</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.025447</td>\n",
              "      <td>0.031011</td>\n",
              "      <td>0.976938</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.025206</td>\n",
              "      <td>0.030775</td>\n",
              "      <td>0.976938</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.024976</td>\n",
              "      <td>0.030550</td>\n",
              "      <td>0.976938</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.024756</td>\n",
              "      <td>0.030335</td>\n",
              "      <td>0.976938</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.024545</td>\n",
              "      <td>0.030128</td>\n",
              "      <td>0.977429</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.024342</td>\n",
              "      <td>0.029929</td>\n",
              "      <td>0.977429</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.024148</td>\n",
              "      <td>0.029738</td>\n",
              "      <td>0.977429</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.023960</td>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.977429</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}